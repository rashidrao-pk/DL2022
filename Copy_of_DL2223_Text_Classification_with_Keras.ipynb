{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashidrao-pk/DL2022/blob/main/Copy_of_DL2223_Text_Classification_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wr74bcd9ILz",
        "outputId": "af41ac94-6e77-450f-a1ab-196911237362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-21 15:24:00--  https://datacloud.di.unito.it/index.php/s/jc6xY3cGDgMcdpY/download/train.csv\n",
            "Resolving datacloud.di.unito.it (datacloud.di.unito.it)... 130.192.156.25\n",
            "Connecting to datacloud.di.unito.it (datacloud.di.unito.it)|130.192.156.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1598506 (1.5M) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   1.52M  1.35MB/s    in 1.1s    \n",
            "\n",
            "2023-02-21 15:24:01 (1.35 MB/s) - ‘train.csv’ saved [1598506/1598506]\n",
            "\n",
            "--2023-02-21 15:24:01--  https://datacloud.di.unito.it/index.php/s/LHfgiHipMy95nQr/download/dev.csv\n",
            "Resolving datacloud.di.unito.it (datacloud.di.unito.it)... 130.192.156.25\n",
            "Connecting to datacloud.di.unito.it (datacloud.di.unito.it)|130.192.156.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226062 (221K) [text/csv]\n",
            "Saving to: ‘dev.csv’\n",
            "\n",
            "dev.csv             100%[===================>] 220.76K  1.18MB/s    in 0.2s    \n",
            "\n",
            "2023-02-21 15:24:02 (1.18 MB/s) - ‘dev.csv’ saved [226062/226062]\n",
            "\n",
            "--2023-02-21 15:24:02--  https://datacloud.di.unito.it/index.php/s/oqAWsFmZMegMzHG/download/test.csv\n",
            "Resolving datacloud.di.unito.it (datacloud.di.unito.it)... 130.192.156.25\n",
            "Connecting to datacloud.di.unito.it (datacloud.di.unito.it)|130.192.156.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 135407 (132K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 132.23K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-02-21 15:24:02 (1.67 MB/s) - ‘test.csv’ saved [135407/135407]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Download data: AbusEval (https://github.com/tommasoc80/AbuseEval)\n",
        "!wget -c https://datacloud.di.unito.it/index.php/s/jc6xY3cGDgMcdpY/download/train.csv\n",
        "!wget -c https://datacloud.di.unito.it/index.php/s/LHfgiHipMy95nQr/download/dev.csv\n",
        "!wget -c https://datacloud.di.unito.it/index.php/s/oqAWsFmZMegMzHG/download/test.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!title Download SpaCy model for English\n",
        "import locale\n",
        "import spacy\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "locale.getpreferredencoding()\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsi15n0-ZE3U",
        "outputId": "adfae9d9-59de-431c-dfd1-0df1107acff0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 15:26:21.309442: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-21 15:26:21.309567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-21 15:26:21.309587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmEB3j8oARFq",
        "outputId": "ea8453c8-393a-43b0-f1aa-3c60c80e9c66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions for text preprocessing\n",
        "import spacy\n",
        "import csv\n",
        "import preprocessor as tp\n",
        "from tqdm import tqdm\n",
        "\n",
        "tp.set_options(tp.OPT.URL, tp.OPT.EMOJI, tp.OPT.HASHTAG, tp.OPT.MENTION)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"pos\"])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = tp.clean(text)\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text.lower() for token in doc if (not token.is_stop)]\n",
        "    return tokens\n",
        "\n",
        "def preprocess_csv(filename):\n",
        "    sentences = []\n",
        "    ids = []\n",
        "    labels = []\n",
        "    with open(filename, encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in tqdm(reader):\n",
        "            sentences.append(preprocess_text(row[\"text\"]))\n",
        "            ids.append(row[\"id\"])\n",
        "            labels.append(eval(row[\"label\"]))\n",
        "    return sentences, ids, labels"
      ],
      "metadata": {
        "id": "ar54aWFN_KDA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train, ids_train, labels_train = preprocess_csv(\"train.csv\")\n",
        "sentences_dev, ids_dev, labels_dev = preprocess_csv(\"dev.csv\")\n",
        "sentences_test, ids_test, labels_test = preprocess_csv(\"test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQmzCd3K9-H6",
        "outputId": "89807294-1575-4827-94d0-a9f35b41e1a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "11585it [01:31, 127.09it/s]\n",
            "1655it [00:12, 127.89it/s]\n",
            "860it [00:06, 134.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (sentences_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6926VO61EXWH",
        "outputId": "84186cbf-aeb4-4ba0-c8ca-a663f6a8dcf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['established', 'war', 'living', '.', 's', 'nt', 'happening']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer(filters='', lower=True, split=' ')\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(sentences_train), 50)\n",
        "y_train = np.array(labels_train)\n",
        "X_dev = pad_sequences(tokenizer.texts_to_sequences(sentences_dev), 50)\n",
        "y_dev = np.array(labels_dev)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(sentences_test), 50)\n",
        "y_test = np.array(labels_test)"
      ],
      "metadata": {
        "id": "P099uDIsB1KK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (X_train.shape)\n",
        "print (y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VtHeOkrFiE7",
        "outputId": "628d611e-55a5-45df-ab81-582e2536de0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11585, 50)\n",
            "(11585,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(\n",
        "        len(word_index)+1, \n",
        "        100, \n",
        "        input_shape=(50,)))\n",
        "model.add(LSTM(8))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# model.add(Dense(2, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "FiFb8oeUF8eD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import adam_v2\n",
        "\n",
        "opt = adam_v2.Adam(learning_rate=1e-4)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['acc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl00-CfBGkOS",
        "outputId": "c15aa5df-e9c1-43db-de10-1ea07f4e9ee1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 50, 100)           1683900   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 8)                 3488      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,687,397\n",
            "Trainable params: 1,687,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    shuffle=True,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_dev, y_dev))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlMKqOZ2Gnsq",
        "outputId": "679e0fb4-357d-4549-f446-b9e972eb23fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "363/363 [==============================] - 11s 12ms/step - loss: 0.5923 - acc: 0.7657 - val_loss: 0.5067 - val_acc: 0.7921\n",
            "Epoch 2/5\n",
            "363/363 [==============================] - 3s 8ms/step - loss: 0.4976 - acc: 0.7924 - val_loss: 0.4957 - val_acc: 0.7921\n",
            "Epoch 3/5\n",
            "363/363 [==============================] - 3s 7ms/step - loss: 0.4810 - acc: 0.7924 - val_loss: 0.4850 - val_acc: 0.7927\n",
            "Epoch 4/5\n",
            "363/363 [==============================] - 4s 10ms/step - loss: 0.4565 - acc: 0.7934 - val_loss: 0.4711 - val_acc: 0.7927\n",
            "Epoch 5/5\n",
            "363/363 [==============================] - 3s 8ms/step - loss: 0.4160 - acc: 0.8068 - val_loss: 0.4526 - val_acc: 0.7964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = [int(x>=0.5) for x in predictions]\n",
        "print (classification_report(y_test, predicted_classes))"
      ],
      "metadata": {
        "id": "51p4Yh93HDL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title let's make it more interesting: more layers, class weights\n",
        "from keras.layers import Bidirectional, GRU, SimpleRNN\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index)+1, 100, input_shape=(50,)))\n",
        "model.add(Bidirectional(GRU(8)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['acc'])\n",
        "\n",
        "cw = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train), \n",
        "    y=y_train)\n",
        "cw = dict(enumerate(cw))\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    shuffle=True,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_dev, y_dev),\n",
        "                    class_weight=cw)\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = [int(x>=0.5) for x in predictions]\n",
        "print (classification_report(y_test, predicted_classes))"
      ],
      "metadata": {
        "id": "Y1GU6Z1NILmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot the learning curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AD0YObLXchz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pre-trained embeddings (fromhttps://fasttext.cc/docs/en/crawl-vectors.html)\n",
        "\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "\n",
        "import gzip\n",
        "\n",
        "embeddings_index = {}\n",
        "with gzip.open('cc.en.300.vec.gz', 'rt', encoding=\"utf-8\") as f:\n",
        "    for line in tqdm(f):\n",
        "        values = line.strip().split()\n",
        "        if len(values)<=2:\n",
        "            continue\n",
        "        word = \" \".join(values[:-300])\n",
        "        coefs = np.asarray(values[-300:], dtype='float32')\n",
        "        embeddings_index[word.lower().strip()] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n"
      ],
      "metadata": {
        "id": "Ekb-CkdaLIzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classification with pre-trained embeddings (frozen)\n",
        "model = Sequential()\n",
        "model.add(Embedding(\n",
        "    len(word_index)+1,\n",
        "    300,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False,\n",
        "    input_shape=(50,)))\n",
        "model.add(Bidirectional(GRU(8)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['acc'])\n",
        "\n",
        "cw = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train), \n",
        "    y=y_train)\n",
        "cw = dict(enumerate(cw))\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    shuffle=True,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_dev, y_dev),\n",
        "                    class_weight=cw)\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = [int(x>=0.5) for x in predictions]\n",
        "print (classification_report(y_test, predicted_classes))\n"
      ],
      "metadata": {
        "id": "JGDO_OdIN48N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T6kLm9rMdA6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classification with pre-trained embeddings (continuing the training)\n",
        "model = Sequential()\n",
        "model.add(Embedding(\n",
        "    len(word_index)+1,\n",
        "    300,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True,\n",
        "    input_shape=(50,)))\n",
        "model.add(Bidirectional(GRU(8)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['acc'])\n",
        "cw = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train), \n",
        "    y=y_train)\n",
        "cw = dict(enumerate(cw))\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    shuffle=True,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_dev, y_dev),\n",
        "                    class_weight=cw)\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = [int(x>=0.5) for x in predictions]\n",
        "print (classification_report(y_test, predicted_classes))\n"
      ],
      "metadata": {
        "id": "luKuVPu1M88X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q6Wi6MdsVPZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Keras Functional API\n",
        "from keras.layers import Input\n",
        "from keras import Model\n",
        "\n",
        "input = Input(shape=(50,))\n",
        "emb = Embedding(\n",
        "    len(word_index)+1,\n",
        "    300,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True,\n",
        "    input_shape=(50,))(input)\n",
        "gru = Bidirectional(GRU(8))(emb)\n",
        "output = Dense(1, activation='sigmoid')(gru)\n",
        "\n",
        "model = Model(inputs=input, outputs=output)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['acc'])\n",
        "cw = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train), \n",
        "    y=y_train)\n",
        "cw = dict(enumerate(cw))\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    shuffle=True,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_dev, y_dev),\n",
        "                    class_weight=cw)\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = [int(x>=0.5) for x in predictions]\n",
        "print (classification_report(y_test, predicted_classes))"
      ],
      "metadata": {
        "id": "6lEmBsTh3JMt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}